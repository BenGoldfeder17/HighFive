import os
import numpy as np
import tensorflow as tf
from tensorflow.keras import Input, Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Flatten, Dense, Dropout

# ------------------------------------------------------
# PARAMETERS & DIRECTORY SETUP
# ------------------------------------------------------
dataset_dir    = "Users/Blake/Documents/GEEN1400/trashnet-enhance"
img_height     = 150
img_width      = 150
batch_size     = 16
epochs         = 30
learning_rate  = 1e-4

# ------------------------------------------------------
# FIXED CLASS LIST (7 classes)
# ------------------------------------------------------
classes = [
    "biodegradable",
    "cardboard",
    "glass",
    "metal",
    "plastic",
    "paper",
    "other_trash"
]
class_to_label = {cls: idx for idx, cls in enumerate(classes)}

# ------------------------------------------------------
# GUARD: ensure each class folder exists
# ------------------------------------------------------
for cls in classes:
    folder = os.path.join(dataset_dir, cls)
    if not os.path.isdir(folder):
        raise FileNotFoundError(f"Expected folder for class '{cls}' not found at {folder}")

# ------------------------------------------------------
# COLLECT IMAGE PATHS & LABELS
# ------------------------------------------------------
all_paths, all_labels = [], []
for cls in classes:
    folder = os.path.join(dataset_dir, cls)
    for fname in os.listdir(folder):
        ext = os.path.splitext(fname)[1].lower()
        if ext not in {'.jpg', '.jpeg', '.png'}:
            continue
        all_paths.append(os.path.join(folder, fname))
        all_labels.append(class_to_label[cls])

if not all_paths:
    raise RuntimeError(f"No images found in any class folders under '{dataset_dir}'")
print(f"Found {len(all_paths)} images across classes: {classes}")

# ------------------------------------------------------
# TRAIN/VAL/TEST SPLIT (70/15/15)
# ------------------------------------------------------
data = list(zip(all_paths, all_labels))
np.random.shuffle(data)
paths, labels = zip(*data)

n       = len(paths)
n_train = int(0.7 * n)
n_val   = int(0.15 * n)

train_paths  = paths[:n_train]
train_labels = labels[:n_train]
val_paths    = paths[n_train:n_train + n_val]
val_labels   = labels[n_train:n_train + n_val]
test_paths   = paths[n_train + n_val:]
test_labels  = labels[n_train + n_val:]

print(f"Split â†’ Train:{len(train_paths)}, Val:{len(val_paths)}, Test:{len(test_paths)}")

# ------------------------------------------------------
# DATASET CREATION
# ------------------------------------------------------
def load_and_preprocess(path, label):
    img_bytes = tf.io.read_file(path)
    img = tf.image.decode_image(img_bytes, channels=3, expand_animations=False)
    img = tf.image.resize(img, [img_height, img_width])
    img = img / 255.0
    return img, label


def make_dataset(paths, labels, shuffle=True):
    ds = tf.data.Dataset.from_tensor_slices((list(paths), list(labels)))
    ds = ds.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)
    ds = ds.apply(tf.data.experimental.ignore_errors())
    if shuffle:
        ds = ds.shuffle(buffer_size=len(paths))
    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)
    return ds

train_ds = make_dataset(train_paths, train_labels, shuffle=True)
val_ds   = make_dataset(val_paths,   val_labels,   shuffle=False)
test_ds  = make_dataset(test_paths,  test_labels,  shuffle=False)

# ------------------------------------------------------
# MODEL DEFINITION
# ------------------------------------------------------
model = Sequential([
    Input(shape=(img_height, img_width, 3)),
    Conv2D(32, (3,3)), Activation('relu'), MaxPooling2D(),
    Conv2D(64, (3,3)), Activation('relu'), MaxPooling2D(),
    Conv2D(128,(3,3)),Activation('relu'), MaxPooling2D(),
    Flatten(),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(len(classes), activation='softmax')
])

model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=tf.keras.optimizers.Adam(learning_rate),
    metrics=['accuracy']
)
model.summary()

# ------------------------------------------------------
# TRAINING
# ------------------------------------------------------
history = model.fit(
    train_ds,
    epochs=epochs,
    validation_data=val_ds
)

# ------------------------------------------------------
# EVALUATION & SAVE
# ------------------------------------------------------
test_loss, test_acc = model.evaluate(test_ds)
print(f"\nTest Accuracy: {test_acc:.4f}")

model.save("trash_recycling_model.keras")
print("Saved model to trash_recycling_model.keras")

# ------------------------------------------------------
# PREDICTION & BINNING FUNCTION
# ------------------------------------------------------
def predict_and_bin(image_path):
    img = tf.io.read_file(image_path)
    img = tf.image.decode_image(img, channels=3, expand_animations=False)
    img = tf.image.resize(img, [img_height, img_width]) / 255.0
    img = tf.expand_dims(img, axis=0)  # batch dim

    preds = model.predict(img)[0]
    pred_idx = np.argmax(preds)
    fine_label = classes[pred_idx]
    confidence = float(preds[pred_idx])

    # map to trash vs recycling
    if fine_label in ("cardboard", "glass", "metal", "paper"):
        bin_label = "recycling"
    else:
        bin_label = "trash"

    return fine_label, bin_label, confidence

# Example usage:
# fine, binned, conf = predict_and_bin(train_paths[0])
# print(f"Detected '{fine}' (conf={conf:.2f}); put in '{binned}' bin.")
